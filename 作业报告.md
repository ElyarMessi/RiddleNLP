# 作业报告

### 小组成员

伊力亚尔	1800016653

金笑缘		1800016611

方博文		1800018615



### 小组分工

> 这块大家自己补充一下

伊力亚尔：1.数据预处理 	2.dataloader的初步实现 	3.模型架构的初步实现

方博文：1.dataloader的调整  2.模型调参训练

金笑缘：1.模型调参训练  2.尝试相似度预测方法及训练


### 调用的工具

本实验采用哈工大版本的预训练模型 Bert 来实现对谜语的语意理解

具体实现上采用了 huggingface.co 提供的 transformers 包，该预训练模型的地址为

https://huggingface.co/hfl/chinese-bert-wwm-ext


### 实验方法

1. 主要思路是基于bert的句子对任务思想（即输入一对句子，输出代表两个句子间的关联性的 [CLS]）

2. 将一个谜语分解为五个样本
   - 每个样本中的 x 是 `[CLS] + “谜语” + “谜语提示” + [SEP] + "候选项" + “候选项的wiki解释”`
   - 每个样本中的 y 是一个标签，代表这个候选项是否正确

3. 得到了五个候选项与谜语encoding之后的输出后，softmax计算得分，prediction即为得分最高者
   - 即不打乱数据样本，每五个样本为一组batch数据
   - 在 Bert 的 pooled_out 输出的基础上接两个 linear 层

### 结果比较与分析

实验参数设置请参考我们的代码，实验结果如下。

以训练过程中验证集上表现最好的模型为准：
1. 训练集上Accuracy 83.29% ( 3330 / (3330 + 668) )
2. 验证集上Accuracy 57.2% ( 286 / (286 + 214) )

### 其他尝试

除了对五个选项生成的对应句子对的合理性进行比较之外，还尝试了如下两种方法

- 直接分别单独预测五个样本的合理性，这样的问题是严重的数据不平衡，性能不佳。
- 用谜语分别和五个选项在Encode之后计算相似度，由于相似度使用向量内积，并不能很好应用到谜语的解题场景中，性能不佳。

### 想法

> 教学网上要求写想法，感觉可有可无

方博文：一开始没想到这个方法能成，后来没想到模型比我厉害
金笑缘：三种方法其中只有一种效果较好，谜语问题是很有趣的非典型QA任务，后续会继续关注
