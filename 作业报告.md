# 作业报告

### 小组成员

伊力亚尔	1800016653

金笑缘		1800016611

方博文		1800018615



### 小组分工

> 这块大家自己补充一下

伊力亚尔：1.数据预处理 	2.dataloader的初步实现 	3.模型架构的初步实现

金笑缘：

方博文：1.dataloader的调整  2.模型调参训练



### 调用的工具

调用huggingface中哈工大版本的bert：

https://huggingface.co/hfl/chinese-bert-wwm-ext



### 实验方法

> 这部分大家也可以补充修正

1.大体思路是利用bert的句子对任务（即输入一对句子，输出这两个句子间的关联性）

2.将一个谜语分解为五个样本，每个样本中的 x 是 [CLS]+ “谜语” 拼接 “谜语提示” +[SEP]+ "候选项" 拼接 “候选项的wiki解释”，y 是一个标签，代表这个候选项是否正确

3.得到了五个候选项与谜语encoding之后的输出后，softmax计算得分，prediction即为得分最高者



### 结果比较与分析

> 这部分等模型最后的结果再写
以训练过程中验证集上表现最好的模型为准：
1. 训练集上Accuracy 83.29% ( 3330 / (3330 + 668) )
2. 验证集上Accuracy 57.2% ( 286 / (286 + 214) )



### 想法

> 教学网上要求写想法，感觉可有可无
方博文：一开始没想到这个方法能成，后来没想到模型比我厉害
